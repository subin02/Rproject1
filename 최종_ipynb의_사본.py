# -*- coding: utf-8 -*-
"""최종.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BZVtcCt7L_WXfJR5dbr7eq2rT-kAmQH1
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sur = pd.read_csv('/content/drive/MyDrive/캡스톤/surface_tp.csv')

sur.info()

for i in sur.index:
 if sur.loc[i,'ta'] == -99.9:
    sur.loc[i,'ta'] = None
 if sur.loc[i,'td'] == -99.9:
    sur.loc[i,'td'] = None
 if sur.loc[i,'hm'] == -99.9:
    sur.loc[i,'hm'] = None
 if sur.loc[i,'ws'] == -99.9:
    sur.loc[i,'ws'] = None
 if sur.loc[i,'rn'] == -99.9:
    sur.loc[i,'rn'] = None
 if sur.loc[i,'re'] == -99:
    sur.loc[i,'re'] = None
 if sur.loc[i,'ts'] == -99.9:
    sur.loc[i,'ts'] = None
 if sur.loc[i,'si'] == -99.9:
    sur.loc[i,'si'] = None
 if sur.loc[i,'ss'] == -99.9:
    sur.loc[i,'ss'] = None

left = []

mmddhh = sur['mmddhh'].to_list()
for m in mmddhh:
  str_m = str(m)
  if len(str_m) == 5:
    v = str_m[0]
  elif len(str_m) == 6:
    v = str_m[:2]
  left.append(v)

np.unique(left, return_counts=True)

"""표준화"""

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

q = []
for L in left:
  int_L = int(L)
  if (int_L <= 3):
    quart = 1
  elif (4 <= int_L <= 6):
    quart = 2
  elif (7 <= int_L <= 9):
    quart = 3
  elif (int_L >= 10):
    quart = 4
  q.append(quart)

sur['q'] = q

np.unique(sur['q'].to_list(), return_counts=True)

sur.info()

sur.head()

# 더미변수화
dum = pd.get_dummies(sur['q'], prefix='q')
dum.head()

sur = pd.concat([sur, dum], axis=1)
sur.head()

sur.describe()

plt.figure(figsize = (5,10))
sns.boxplot(y = sur['ta'])

sns.displot(data = sur, x='ta', kind ='kde')

plt.figure(figsize = (5,10))
sns.boxplot(y = sur['td'])

sns.displot(data = sur, x='td', kind = 'kde')

sns.displot(data = sur, x='hm', kind = 'kde')

sns.displot(data = sur, x='rn', kind = 'kde')

sns.displot(data = sur, x='re', kind = 'kde')

plt.figure(figsize = (5,10))
sns.boxplot(y = sur['ts'])

sns.displot(data = sur, x='ts', kind = 'kde')

plt.figure(figsize = (50,20))
sns.scatterplot(x = sur['no'], y = sur['ts'])

plt.figure(figsize = (5,10))
sns.boxplot(y = sur['si'])

sns.displot(data = sur, x='si', kind='kde')

sns.displot(data = sur, x='ss', kind = 'kde')

sur.columns

sur.drop('stn', axis = 1, inplace = True)
sur.drop('year', axis = 1, inplace = True)
sur.drop('mmddhh', axis = 1, inplace = True)

"""# 결측치 제거"""

sur1 = sur.copy()

sur1.isnull().sum()

sur1 = sur1.dropna(axis=0)

sur1.isnull().sum()

sur.columns

# 더미변수 제외하고 표준화
x1 = sur1[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'q']]
y1 = sur1['ts']

from sklearn.model_selection import train_test_split
x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, random_state = 1234)

dum1_train = pd.get_dummies(x1_train['q'], prefix='q')
dum1_train.head()

dum1_test = pd.get_dummies(x1_test['q'], prefix='q')
dum1_test.head()

x1_train.drop('q', axis = 1, inplace = True)
x1_test.drop('q', axis = 1, inplace = True)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

ss.fit(x1_train)
ss.fit(x1_test)

x1_train = ss.transform(x1_train) #transform(): 데이터 변환
x1_test = ss.transform(x1_test)

x1_train

# 더미변수와 합치기
x1_train = np.concatenate((x1_train, np.array(dum1_train)), axis=1)
x1_test = np.concatenate((x1_test, np.array(dum1_test)), axis=1)

x1_train

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error

sur1.columns

df

df = pd.DataFrame(x1_train, columns = [['ta','td','hm','ws','rn','re','si','ss','q_1','q_2','q_3','q_4']])

# 전진 단계별 선택법
import statsmodels.api as sm
variables = df.columns.tolist()

y = y1_train.values
selected_variables2 = [] ## 선택된 변수들
sl_enter = 0.05 # 기준 유의확률
sl_remove = 0.05

sv_per_step = [] ## 각 스텝별로 선택된 변수들
adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수
steps = [] ## 스텝
step = 0
while len(variables) > 0:
    remainder = list(set(variables) - set(selected_variables2))
    pval = pd.Series(index=remainder) ## 변수의 p-value
    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서
    ## 선형 모형을 적합한다.
    for col in remainder:
        X = df[selected_variables2+[col]]
        X = sm.add_constant(X)
        model = sm.OLS(y,X).fit()
        pval[col] = model.pvalues[col]

    min_pval = pval.min()
    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함
        selected_variables2.append(pval.idxmin())
        ## 선택된 변수들에대해서
        ## 어떤 변수를 제거할지 고른다.
        while len(selected_variables2) > 0:
            selected_X = df[selected_variables2]
            selected_X = sm.add_constant(selected_X)
            selected_pval = sm.OLS(y,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다
            max_pval = selected_pval.max()
            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외
                remove_variable = selected_pval.idxmax()
                selected_variables2.remove(remove_variable)
            else:
                break

        step += 1
        steps.append(step)
        adj_r_squared = sm.OLS(y,sm.add_constant(df[selected_variables2])).fit().rsquared_adj
        adjusted_r_squared.append(adj_r_squared)
        sv_per_step.append(selected_variables2.copy())
    else:
        break

print(selected_variables2)

# 단순회귀
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(x1_train, y1_train)

print(lr.score(x1_train, y1_train))
print(lr.score(x1_test, y1_test))

#MAE
pred_lr = lr.predict(x_test)
print(mean_absolute_error(y_test, pred_lr))

#MSE

print(mean_squared_error(y_test, pred_lr))

#RMSE
MSE = mean_squared_error(y_test, pred_lr)
print(np.sqrt(MSE))

# 의사결정나무
from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor()
dtr.fit(x_train, y_train)

print(dtr.score(x_train, y_train))
print(dtr.score(x_test, y_test))

pred_dtr = dtr.predict(x_test)

#MAE
print(mean_absolute_error(y_test, pred_dtr))

#MSE

print(mean_squared_error(y_test, pred_dtr))

#RMSE
MSE = mean_squared_error(y_test, pred_dtr)
print(np.sqrt(MSE))

#MAPE
print(mean_absolute_percentage_error(y_test, pred_dtr))

# random forest
from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor()
rfr.fit(x_train, y_train)

print(rfr.score(x_train, y_train))
print(rfr.score(x_test, y_test))

pred_rfr = rfr.predict(x_test)

#MAE
print(mean_absolute_error(y_test, pred_rfr))

#MSE

print(mean_squared_error(y_test, pred_rfr))

#RMSE
MSE = mean_squared_error(y_test, pred_rfr)
print(np.sqrt(MSE))

# xgboost
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import *

xgb = xgb.XGBRegressor()

xgb.fit(x_train, y_train)

print(xgb.score(x_train, y_train))
print(xgb.score(x_test, y_test))

pred_xgb = xgb.predict(x_test)

#MAE
print(mean_absolute_error(y_test, pred_xgb))

#MSE

print(mean_squared_error(y_test, pred_xgb))

#RMSE
MSE = mean_squared_error(y_test, pred_xgb)
print(np.sqrt(MSE))



"""# 결측치 대체 - 보간(+ 보간 후 남은 데이터 삭제)"""

sur2 = sur.copy()

sur2.info()

sur2['ta'].interpolate(inplace = True)
sur2['td'].interpolate(inplace = True)
sur2['hm'].interpolate(inplace = True)
sur2['ws'].interpolate(inplace = True)
sur2['rn'].interpolate(inplace = True)
sur2['re'].interpolate(inplace = True)
sur2['ts'].interpolate(inplace = True)
sur2['si'].interpolate(inplace = True)
sur2['ss'].interpolate(inplace = True)

sur2.isnull().sum()

sur2[sur2['si'].isnull()]

for i in range(0,8):
 sur2 = sur2.drop(i, axis=0)

sur2.isnull().sum()

x2 = sur2[['ta', 'td', 'hm', 'ws', 'rn', 're', 'si', 'ss', 'q_1', 'q_2', 'q_3', 'q_4']]
y2 = sur2['ts']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x21 = scaler.fit_transform(x2)


from sklearn.model_selection import train_test_split
x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, train_size = 0.7, random_state = 1234)

x2

x21 = pd.DataFrame(x21, columns= x2.columns)
x21

import statsmodels.api as sm
variables = x21


selected_variables2 = [] ## 선택된 변수들
sl_enter = 0.05 # 기준 유의확률
sl_remove = 0.05

sv_per_step = [] ## 각 스텝별로 선택된 변수들
adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수
steps = [] ## 스텝
step = 0
while len(variables) > 0:
    remainder = list(set(variables) - set(selected_variables2))
    pval = pd.Series(index=remainder) ## 변수의 p-value
    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서
    ## 선형 모형을 적합한다.
    for col in remainder:
        X = sur2[selected_variables2+[col]]
        X = sm.add_constant(X)
        model = sm.OLS(y2,X).fit()
        pval[col] = model.pvalues[col]

    min_pval = pval.min()
    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함
        selected_variables2.append(pval.idxmin())
        ## 선택된 변수들에대해서
        ## 어떤 변수를 제거할지 고른다.
        while len(selected_variables2) > 0:
            selected_X = sur2[selected_variables2]
            selected_X = sm.add_constant(selected_X)
            selected_pval = sm.OLS(y2,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다
            max_pval = selected_pval.max()
            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외
                remove_variable = selected_pval.idxmax()
                selected_variables2.remove(remove_variable)
            else:
                break

        step += 1
        steps.append(step)
        adj_r_squared = sm.OLS(y2,sm.add_constant(sur2[selected_variables2])).fit().rsquared_adj
        adjusted_r_squared.append(adj_r_squared)
        sv_per_step.append(selected_variables2.copy())
    else:
        break

selected_variables2

# 단순회귀
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(x2_train, y2_train)

print(lr.score(x2_train, y2_train))
print(lr.score(x2_test, y2_test))

plt.figure(figsize=(15,7))
plt.plot(lr.predict(x2_test[:100]), label='predict')
plt.plot(y2_test[:100].values.reshape(-1,1), label = 'real')
plt.legend()

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error

#MAE
pred_lr = lr.predict(x2_test)
print(mean_absolute_error(y2_test, pred_lr))

#MSE

print(mean_squared_error(y2_test, pred_lr))

#RMSE
MSE = mean_squared_error(y2_test, pred_lr)
print(np.sqrt(MSE))

# 의사결정나무
from sklearn.tree import DecisionTreeRegressor

dtr = DecisionTreeRegressor()
dtr.fit(x2_train, y2_train)

print(dtr.score(x2_train, y2_train))
print(dtr.score(x2_test, y2_test))

plt.figure(figsize=(15,7))
plt.plot(dtr.predict(x2_test[:100]), label='predict')
plt.plot(y2_test[:100].values.reshape(-1,1), label = 'real')
plt.legend()

pred_dtr = dtr.predict(x2_test)

#MAE
print(mean_absolute_error(y2_test, pred_dtr))

#MSE

print(mean_squared_error(y2_test, pred_dtr))

#RMSE
MSE = mean_squared_error(y2_test, pred_dtr)
print(np.sqrt(MSE))

# random forest
from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor()
rfr.fit(x2_train, y2_train)

print(rfr.score(x2_train, y2_train))
print(rfr.score(x2_test, y2_test))

plt.figure(figsize=(15,7))
plt.plot(rfr.predict(x2_test[:100]), label='predict')
plt.plot(y2_test[:100].values.reshape(-1,1), label = 'real')
plt.legend()

pred_rfr = rfr.predict(x2_test)

#MAE
print(mean_absolute_error(y2_test, pred_rfr))

#MSE

print(mean_squared_error(y2_test, pred_rfr))

#RMSE
MSE = mean_squared_error(y2_test, pred_rfr)
print(np.sqrt(MSE))

# xgboost
import xgboost as xgb

xgb = xgb.XGBRegressor()
xgb.fit(x2_train, y2_train)

print(xgb.score(x2_train, y2_train))
print(xgb.score(x2_test, y2_test))

plt.figure(figsize=(15,7))
plt.plot(xgb.predict(x2_test[:100]), label='predict')
plt.plot(y2_test[:100].values.reshape(-1,1), label = 'real')
plt.legend()

pred_xgb = xgb.predict(x2_test)

#MAE
print(mean_absolute_error(y2_test, pred_xgb))

#MSE

print(mean_squared_error(y2_test, pred_xgb))

#RMSE
MSE = mean_squared_error(y2_test, pred_xgb)
print(np.sqrt(MSE))

"""# 독립변수 확인"""

plt.figure(figsize=(20,10))

sns.heatmap(sur1.corr(), annot = True)

plt.figure(figsize=(20,10))

sns.heatmap(sur2.corr(), annot=True)

